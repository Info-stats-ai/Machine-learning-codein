{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85c2492-7e5f-4c80-a8f9-abe6ece18832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc72770b-f4f2-4ce7-b1f8-0a49823ce761",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "- Corpora -->> It is  collection of texts\n",
    "- Lexical resources\n",
    "- It provides you lot of resources and datasets for learning part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0e13e-df64-4cb6-894c-2de94433f921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5d00d5-3adf-44f6-a443-ae1324f68715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240eca01-2644-49da-8b91-1e541deae69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "# After executing this , command line pop up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf119f9-c49d-4601-a3d2-ee59a7dfa127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ce3c98-be95-4ebc-8c2f-92a85e58b8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.util.LazyCorpusLoader"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f64268-3512-4a00-a3af-26c8c484246f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n",
    "# gives data of different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "678e4f7b-7647-4673-86f9-16de23d6c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take an example as adventure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6827809e-c26b-490a-a63f-944f39eda809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(brown.categories()))\n",
    "# 15 categories in brown method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "933bbcf9-fab8-4f4c-9aa9-c76999346230",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories='adventure')\n",
    "# get the sentences from the adventure category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd468a6c-fdea-49aa-826b-2f76a4424612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...],\n",
       " nltk.corpus.reader.util.ConcatenatedCorpusView)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3600334d-d672-4c83-a5f2-dc1481a25f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f87669c-ffe9-46b6-8885-09136c0d666c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4637"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101edaa1-a7f8-4d91-aed3-1b44fd281b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'was', 'well', 'rid', 'of', 'her', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbb967bb-f40a-44e3-89a7-ef20af2806f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was well rid of her .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(data[1]) # first sentence in adventure category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d2a986-8610-4f71-abd3-18ff0012e77e",
   "metadata": {},
   "source": [
    "### Bag of Words Pipeline: Text into numbers and then to classifier\n",
    "\n",
    "\n",
    "#### Get the data/Corpus\n",
    "\n",
    "- Real life dataset or nltk.corpus\n",
    "     \n",
    "#### Tokenisation, Stopward Removal\n",
    "\n",
    "- Tokenisation : Process of breaking documents into sentences and then to words.                                                              \n",
    "\n",
    "- Words are the most minimal token which has meaning.\n",
    "\n",
    "- Stopward removal : - ex 1)I like movie, 2)I was running, 3) I had a good sleep. All three examples does not contain very meaning full words like - I, was, like, good as these words does not contain much information.\n",
    "\n",
    "- These words are stop ward words -->> we try to discard these words\n",
    "\n",
    "#### Stemming/Lemmatization\n",
    "  \n",
    "-  To convert different form of words into a base word.\n",
    "   \n",
    "-  Ex- Dog is running and dog runs on street.\n",
    "  \n",
    "-  stemming convert this in base sentence.\n",
    " \n",
    "  \n",
    "### Building a Vocab\n",
    "\n",
    "- List of unique words ; It is a list of after doing stemming , tokenization and stopward removal.\n",
    "  \n",
    "- List of this unique word is called vocab.\n",
    "\n",
    "  \n",
    "### Vectorization\n",
    "\n",
    "- We will make a vector of all zeroes with length == len(vocab-ist).\n",
    "  \n",
    "- What we will do is : we update our vector's index corresponding to the vocab's index where some random word is avaliable by replacing zero with 1.\n",
    "  \n",
    "- Example : - let suppose word \"I\" is found in vocab list at 50th index ,so we will be updating 50th index of vector to 1.\n",
    "  \n",
    "- This is how we convert ourt words into vectors, this process is called vectorization.\n",
    "  \n",
    "- Size of vector = size of list vocab  > size of sentence\n",
    "  \n",
    "- Order of words doesnot matter\n",
    "  \n",
    "\n",
    "  \n",
    "### Classification\n",
    "- finally we classify\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec87fd-b594-4bc7-a0cf-ed07d01ff671",
   "metadata": {},
   "source": [
    "### Tokenisation and stopword Removal\n",
    "- Doc to Sentences to words.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8395756-a59e-4b52-93ae-2931b1c9773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\" hey hy how are you , not preety much going on, what about you \n",
    "what you did till now\"\"\"\n",
    "\n",
    "example = 'Send all the 50 documents related to chapters 1,2,3 at prateek@cn.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b70c7f-e213-4299-8a49-6bd5091a7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "# sent_tokenize-->> breaks doc into sentences.\n",
    "# and word_tokenize -->> breaks sentences into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "def2ef64-fc74-402a-8ef8-42e3887d31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "948f4864-b9d1-4217-8b82-f0be6c5ab9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' hey hy how are you , not preety much going on, what about you \\nwhat you did till now']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e01409b-f542-4dc3-9f42-13462d0a2017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(document), type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac20a063-7384-46c5-9244-0d0c3ed5f88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences)) #oth index contains  three sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9468d61f-6c74-4fb2-abb8-6a979688484b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hey hy how are you , not preety much going on, what about you \\nwhat you did till now'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03e79135-cc95-415d-a7d0-eee9d76dc5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,2,3', 'at', 'prateek@cn.com']\n"
     ]
    }
   ],
   "source": [
    "print(example.split())\n",
    "#split funtion is unable to seperate numeric value\n",
    "#'1,2,3' are at 1 single index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7f4a9b7-0767-4978-b588-970a5a0f0296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,2,3', 'at', 'prateek', '@', 'cn.com']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example))\n",
    "# it is able to break \"@\" seperately\n",
    "# but still '1,2,3' are at the same index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bf12bf4-2e15-4474-bedb-c19d2d38724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(document) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b15ba6ba-da48-4997-820e-779d2eac4d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'hy', 'how', 'are', 'you', ',', 'not', 'preety', 'much', 'going', 'on', ',', 'what', 'about', 'you', 'what', 'you', 'did', 'till', 'now']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2dbfd370-bd85-4e02-a849-9a3fdf8140c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return a tokenized copy of *text*,\n",
       "using NLTK's recommended word tokenizer\n",
       "(currently an improved :class:`.TreebankWordTokenizer`\n",
       "along with :class:`.PunktSentenceTokenizer`\n",
       "for the specified language).\n",
       "\n",
       ":param text: text to split into words\n",
       ":type text: str\n",
       ":param language: the model name in the Punkt corpus\n",
       ":type language: str\n",
       ":param preserve_line: A flag to decide whether to sentence tokenize the text or not.\n",
       ":type preserve_line: bool\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_tokenize?\n",
    "# preserve_line= False -->> It is the parameters.\n",
    "# Returns a tokenized copy of texts -->> means a list of words\n",
    "# Model works according to the punkt corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479548dd-b2c0-45fa-b9c0-903166739694",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "- Either we can create our own set of words which we want to remove.\n",
    "- Or we can have it from nltk tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97bbd8b7-e05d-43f3-86a9-86dfa0709d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# stopwords works for different language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb1dd8f7-efef-4934-9838-9bb4217ec52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34a92abc-fd4a-4243-af0f-af814df0021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not', 'does', 'above', 'below', 'doesn', 'o', 'she', 'during', 'off', 'over', 'nor', 't', \"didn't\", 'me', 'to', 'doing', 'whom', 'for', 'most', 'how', 'hasn', 'in', 'himself', 'on', 'out', 'there', 'such', 'this', 'themselves', 'any', \"isn't\", 'against', 'what', 'are', 'then', 'don', 'mightn', 're', 'again', 'needn', 'be', 'other', \"should've\", 'you', 'do', 'until', 'before', 'yours', 'of', 'had', 'once', \"wouldn't\", \"shouldn't\", 'your', \"that'll\", 'after', 'and', 'his', 'we', 'their', 'but', 'shan', \"don't\", \"it's\", 'he', 'further', 'can', 'into', 'few', 'am', \"you're\", 'both', 'didn', 'should', 'between', 'our', 'myself', 'him', 'the', 'who', 's', 'couldn', 'hers', 'been', 'same', 'because', 'll', 'won', \"couldn't\", 'own', 'as', 'wasn', 'while', 'up', 'it', \"shan't\", \"you'll\", 'theirs', 'being', 'where', \"hasn't\", 'd', 'than', 'so', \"won't\", 'shouldn', 'all', 'having', \"aren't\", 'when', 'itself', 'or', 'has', \"needn't\", 'm', 'y', 'ourselves', 'which', \"haven't\", 'only', 'weren', 'her', 'down', 'yourself', 'these', 'ain', \"hadn't\", 'at', 'i', \"you've\", 'about', 'will', 'ma', 'wouldn', 'why', 'from', \"wasn't\", 'were', 'under', 'each', 'ours', 'now', 'my', 'too', 'no', 've', 'yourselves', \"you'd\", 'isn', 'just', 'aren', 'its', \"weren't\", 'more', 'with', \"mustn't\", 'was', 'have', \"mightn't\", 'them', 'herself', 'very', 'mustn', 'by', 'that', 'an', 'through', 'some', \"she's\", 'hadn', 'haven', 'a', \"doesn't\", 'those', 'they', 'did', 'is', 'if', 'here'}\n"
     ]
    }
   ],
   "source": [
    "print(sw)\n",
    "#These are the words we need to remove from the sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e75a633-133c-4396-bf23-196e297b6b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sw) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b601843f-e7d4-4c38-89db-5e2df7f89445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    useful_text = [ words for words in  text if words not in stopwords]\n",
    "    return useful_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "761aadd5-18ea-43a2-b8c5-f115431d83be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send all the 50 documents related to chapters 1,2,3 at prateek@cn.com\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "182b7d27-ef3d-49b6-92af-3176f679ab1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'e', 'n', ' ', 'l', 'l', ' ', 'h', 'e', ' ', '5', '0', ' ', 'c', 'u', 'e', 'n', ' ', 'r', 'e', 'l', 'e', ' ', ' ', 'c', 'h', 'p', 'e', 'r', ' ', '1', ',', '2', ',', '3', ' ', ' ', 'p', 'r', 'e', 'e', 'k', '@', 'c', 'n', '.', 'c']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(remove_stopwords(example, sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74b48581-e581-443b-881e-0eab5b82be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send all the 50 documents related to chapters 1', '2', '3 at prateek@cn.com']\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords(example.split(','), sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac48471c-4cae-4637-8a0b-fa8f2e39bf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'not', 'worried', 'because', 'I', 'am', 'a', 'very', 'cool', 'guy']\n",
      "['worried', 'I', 'cool', 'guy']\n"
     ]
    }
   ],
   "source": [
    "text = \" i am not worried because I am a very cool guy\".split()\n",
    "# split function converts string into list\n",
    "print((text))\n",
    "useful_text = remove_stopwords(text, sw)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9611e94-c9ae-4524-aa9d-870337c7add9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'not' in sw # not id in stopping word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524fe0e8-86a2-4ca2-a6b5-e0dd6ea0116e",
   "metadata": {},
   "source": [
    "### Tokenization Using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e29d7-69a0-4cba-bf30-2570cf67c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.regexpal.com/ -->. for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4f93956-5ee2-46e1-85b2-6d7564214692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2d10286-1c6a-4214-b1c8-fae2f2c43d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z]+') # go to this site for learn more\n",
    "useful_text = tokenizer.tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "669883f9-2425-4724-9766-cd199418b292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapters', 'at', 'prateek', 'cn', 'com']\n"
     ]
    }
   ],
   "source": [
    "print(useful_text)  # 50 is not there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c171dd3-8225-4503-ac99-e70779efbef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send all the 50 documents related to chapters 1,2,3 at prateek@cn.com\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c55c98-4411-4665-a16a-1f7fff8cc5aa",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "- Process that transform particular words (verbs, Plurals) into their radical form.\n",
    "- Preserve the semantics of the sentence without increasing the no. of unique tokens\n",
    "- Example - Jumps, jumping,jumped, jump ===>>> base word is jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a774c12-d458-401e-a9f0-5d4797b1d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" Foxes love to make jumps. the quick brown fox was seen jumping\n",
    "           over the lovely dog from a 6ft high wall\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "297a1568-db42-4267-b934-db6b1671dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cfc4d4be-8bda-4cd3-930a-8843ebc26e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object of an stemmer \n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08b28df0-930e-437e-8a60-e4799dd6dc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quickli'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"quickly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5910448-e4cb-4645-9cb5-53ac9d983a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"lovely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68fa9de7-3600-44e9-aa11-c43fd8671b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"jumps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e844d9f-7186-4815-bf8d-5657ddd0411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SnowballStemmer -->> multilingual\n",
    "ss = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12902b35-34d0-4bee-b38e-c3f8526cfcfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1305402-eacc-42ee-bdae-376d804d8fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump-'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('jump-ing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b624aa79-7f52-4ea3-8cac-eef068293c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'faith'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem(\"faithful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930baff-b4ae-41d6-8d35-b3eb18484a05",
   "metadata": {},
   "source": [
    "### Lemmanization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "00dfd189-6dde-4af4-b321-71048d68f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b6a98265-af99-4c4e-bab7-665bededc341",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "61479090-55c6-4a82-bad5-aae658c29834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump-ing'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize('jump-ing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3d9a69d-95cc-4df4-8557-ecf2b96a0735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize(\"jumps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c71e1e5-883c-43af-b568-7623e9e80226",
   "metadata": {},
   "source": [
    "### Vectorization of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1a31f90-0129-42b0-bc7f-f6ee49edd4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [ \n",
    "'Indian cricket team will win world cup, says capt. rohit sharma , world cup will be held in barbados'\n",
    "'I will get good placement in upcoming summer says confident Omkar'\n",
    "'The nobel laurate won the hearts of the people'\n",
    "'The movie Raazi is an exciting Indian spy thriller based upon a real story'\n",
    "]\n",
    "#we have four documents ,We create 1 feature vector for each document\n",
    "#vocab = across all the documents we will find the unique words\n",
    "#word \"Indian\" appears at 7th index in vocab-list, then at the 7th index\n",
    "#we will update 1 at the 7th index\n",
    "#here vocab data is an dictionary\n",
    "#automate every work using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e521605f-8abb-4788-b808-47888dd1d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9512f60b-c1dc-4581-bf47-e0d79e2dd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6fb1467c-e777-4da8-93b8-fdaef2683859",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "# fit method going to learn the dictionary\n",
    "# tranform method is going to convert into vectorized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b7ddfd6a-762a-4efb-a7e2-d182d7b512ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x40 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 40 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "40dba16d-e9ec-44a6-b3d6-029680352c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "60160ae6-3a25-45e7-839f-b2b28b7f5632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        csr_matrix\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "(0, 14)\t2\n",
       "           (0, 6)\t1\n",
       "           (0, 31)\t1\n",
       "           (0, 36)\t3\n",
       "           (0, 37)\t1\n",
       "           (0, 39)\t2\n",
       "           (0, 7)\t2\n",
       "           (0, 26)\t2\n",
       "           ( <...> 5)\t1\n",
       "           (0, 0)\t1\n",
       "           (0, 8)\t1\n",
       "           (0, 28)\t1\n",
       "           (0, 33)\t1\n",
       "           (0, 2)\t1\n",
       "           (0, 35)\t1\n",
       "           (0, 24)\t1\n",
       "           (0, 29)\t1\n",
       "\u001b[0;31mFile:\u001b[0m        /opt/anaconda3/lib/python3.11/site-packages/scipy/sparse/_csr.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Compressed Sparse Row matrix\n",
       "\n",
       "This can be instantiated in several ways:\n",
       "    csr_array(D)\n",
       "        with a dense matrix or rank-2 ndarray D\n",
       "\n",
       "    csr_array(S)\n",
       "        with another sparse matrix S (equivalent to S.tocsr())\n",
       "\n",
       "    csr_array((M, N), [dtype])\n",
       "        to construct an empty matrix with shape (M, N)\n",
       "        dtype is optional, defaulting to dtype='d'.\n",
       "\n",
       "    csr_array((data, (row_ind, col_ind)), [shape=(M, N)])\n",
       "        where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n",
       "        relationship ``a[row_ind[k], col_ind[k]] = data[k]``.\n",
       "\n",
       "    csr_array((data, indices, indptr), [shape=(M, N)])\n",
       "        is the standard CSR representation where the column indices for\n",
       "        row i are stored in ``indices[indptr[i]:indptr[i+1]]`` and their\n",
       "        corresponding values are stored in ``data[indptr[i]:indptr[i+1]]``.\n",
       "        If the shape parameter is not supplied, the matrix dimensions\n",
       "        are inferred from the index arrays.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "dtype : dtype\n",
       "    Data type of the matrix\n",
       "shape : 2-tuple\n",
       "    Shape of the matrix\n",
       "ndim : int\n",
       "    Number of dimensions (this is always 2)\n",
       "nnz\n",
       "    Number of stored values, including explicit zeros\n",
       "data\n",
       "    CSR format data array of the matrix\n",
       "indices\n",
       "    CSR format index array of the matrix\n",
       "indptr\n",
       "    CSR format index pointer array of the matrix\n",
       "has_sorted_indices\n",
       "    Whether indices are sorted\n",
       "\n",
       "Notes\n",
       "-----\n",
       "\n",
       "Sparse matrices can be used in arithmetic operations: they support\n",
       "addition, subtraction, multiplication, division, and matrix power.\n",
       "\n",
       "Advantages of the CSR format\n",
       "  - efficient arithmetic operations CSR + CSR, CSR * CSR, etc.\n",
       "  - efficient row slicing\n",
       "  - fast matrix vector products\n",
       "\n",
       "Disadvantages of the CSR format\n",
       "  - slow column slicing operations (consider CSC)\n",
       "  - changes to the sparsity structure are expensive (consider LIL or DOK)\n",
       "\n",
       "Canonical Format\n",
       "    - Within each row, indices are sorted by column.\n",
       "    - There are no duplicate entries.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> from scipy.sparse import csr_array\n",
       ">>> csr_array((3, 4), dtype=np.int8).toarray()\n",
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]], dtype=int8)\n",
       "\n",
       ">>> row = np.array([0, 0, 1, 2, 2, 2])\n",
       ">>> col = np.array([0, 2, 2, 0, 1, 2])\n",
       ">>> data = np.array([1, 2, 3, 4, 5, 6])\n",
       ">>> csr_array((data, (row, col)), shape=(3, 3)).toarray()\n",
       "array([[1, 0, 2],\n",
       "       [0, 0, 3],\n",
       "       [4, 5, 6]])\n",
       "\n",
       ">>> indptr = np.array([0, 2, 3, 6])\n",
       ">>> indices = np.array([0, 2, 2, 0, 1, 2])\n",
       ">>> data = np.array([1, 2, 3, 4, 5, 6])\n",
       ">>> csr_array((data, indices, indptr), shape=(3, 3)).toarray()\n",
       "array([[1, 0, 2],\n",
       "       [0, 0, 3],\n",
       "       [4, 5, 6]])\n",
       "\n",
       "Duplicate entries are summed together:\n",
       "\n",
       ">>> row = np.array([0, 1, 2, 0])\n",
       ">>> col = np.array([0, 1, 1, 0])\n",
       ">>> data = np.array([1, 2, 4, 8])\n",
       ">>> csr_array((data, (row, col)), shape=(3, 3)).toarray()\n",
       "array([[9, 0, 0],\n",
       "       [0, 2, 0],\n",
       "       [0, 4, 0]])\n",
       "\n",
       "As an example of how to construct a CSR matrix incrementally,\n",
       "the following snippet builds a term-document matrix from texts:\n",
       "\n",
       ">>> docs = [[\"hello\", \"world\", \"hello\"], [\"goodbye\", \"cruel\", \"world\"]]\n",
       ">>> indptr = [0]\n",
       ">>> indices = []\n",
       ">>> data = []\n",
       ">>> vocabulary = {}\n",
       ">>> for d in docs:\n",
       "...     for term in d:\n",
       "...         index = vocabulary.setdefault(term, len(vocabulary))\n",
       "...         indices.append(index)\n",
       "...         data.append(1)\n",
       "...     indptr.append(len(indices))\n",
       "...\n",
       ">>> csr_array((data, indices, indptr), dtype=int).toarray()\n",
       "array([[2, 1, 0, 0],\n",
       "       [0, 1, 1, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorized_corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1acd8-4eec-490c-9441-401e287aea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the zeroth sentence the first word is found at 6th index in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bc3a608f-cb42-4fb8-af18-aac619364707",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus_array = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "799dd906-7668-4069-8cf1-394e7af5798f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e709f7d2-5298-4e88-9bc1-d1d6ee8ce2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 2])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus_array[0] # first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8b901e0c-ff9d-4983-9526-9a56abceae20",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectorized_corpus_array[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "vectorized_corpus_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b910f76d-5d04-4514-a827-29fdd54ff29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus_array[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e567680d-64a1-4cbe-aca6-ed98c07dfa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 14,\n",
       " 'cricket': 6,\n",
       " 'team': 31,\n",
       " 'will': 36,\n",
       " 'win': 37,\n",
       " 'world': 39,\n",
       " 'cup': 7,\n",
       " 'says': 26,\n",
       " 'capt': 4,\n",
       " 'rohit': 25,\n",
       " 'sharma': 27,\n",
       " 'be': 3,\n",
       " 'held': 12,\n",
       " 'in': 13,\n",
       " 'barbadosi': 1,\n",
       " 'get': 9,\n",
       " 'good': 10,\n",
       " 'placement': 22,\n",
       " 'upcoming': 34,\n",
       " 'summer': 30,\n",
       " 'confident': 5,\n",
       " 'omkarthe': 20,\n",
       " 'nobel': 18,\n",
       " 'laurate': 16,\n",
       " 'won': 38,\n",
       " 'the': 32,\n",
       " 'hearts': 11,\n",
       " 'of': 19,\n",
       " 'peoplethe': 21,\n",
       " 'movie': 17,\n",
       " 'raazi': 23,\n",
       " 'is': 15,\n",
       " 'an': 0,\n",
       " 'exciting': 8,\n",
       " 'spy': 28,\n",
       " 'thriller': 33,\n",
       " 'based': 2,\n",
       " 'upon': 35,\n",
       " 'real': 24,\n",
       " 'story': 29}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "71c0eac1-08a7-4852-ad9f-f0e183f2882a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "90a55f63-71cd-4656-ad82-c60ec4a4c01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['indian', 'cricket', 'team', 'will', 'win', 'world', 'cup', 'says', 'capt', 'rohit', 'sharma', 'be', 'held', 'in', 'barbadosi', 'get', 'good', 'placement', 'upcoming', 'summer', 'confident', 'omkarthe', 'nobel', 'laurate', 'won', 'the', 'hearts', 'of', 'peoplethe', 'movie', 'raazi', 'is', 'an', 'exciting', 'spy', 'thriller', 'based', 'upon', 'real', 'story'])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "39faea2f-705e-4c21-8d7c-5fcad628e649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24cf6eb-8c1f-48e8-b31f-32ec67bbd4c6",
   "metadata": {},
   "source": [
    "### Reverse Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4ea80089-5edd-4805-914a-ff83e967aa46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 2])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = vectorized_corpus_array[0]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5f57944a-820d-417c-9319-a21305dac946",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 3\n 1 1 2].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cv\u001b[38;5;241m.\u001b[39minverse_transform(numbers)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1453\u001b[0m, in \u001b[0;36mCountVectorizer.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;66;03m# We need CSR format for fast row manipulations.\u001b[39;00m\n\u001b[0;32m-> 1453\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1454\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1456\u001b[0m terms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    906\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    907\u001b[0m         )\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 3\n 1 1 2].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "cv.inverse_transform(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681ba66-d8d5-46e0-9ffa-a407a454b8c3",
   "metadata": {},
   "source": [
    "### More ways to create features\n",
    "- Unigram - every word as a feature\n",
    "- Bigram - 2 words combined as a feature\n",
    "- Trigram\n",
    "- n-gram\n",
    "- TF-IDF Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a6b1ca85-327c-4724-ac49-2f0582131c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = [\"this is good movie\"]\n",
    "sent2 = [\"this is not a good movie\"]\n",
    "#both have good movie \n",
    "# this confuses our classififer \n",
    "# it can predict both movie as good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d9fae3d2-da15-44b4-abf7-9ae289e9750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "179ecbd7-620a-45cc-afc6-1211366396c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = [sent1[0],sent2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c78964f-a167-4e35-95a0-c17140717732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is good movie', 'this is not a good movie']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3bd0a2ca-ab35-4964-b974-09b4280e8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = cv.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "55c14a90-d283-4d98-b1b2-a506d13d27fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        csr_matrix\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "  (0, 4)\t1\n",
       "  (0, 1)\t1\n",
       "  (0, 0)\t1\n",
       "  (0, 2)\t1\n",
       "  (1, 4)\t1\n",
       "  (1, 1)\t1\n",
       "  (1, 0)\t1\n",
       "  (1, 2)\t1\n",
       "  (1, 3)\t1\n",
       "\u001b[0;31mFile:\u001b[0m        /opt/anaconda3/lib/python3.11/site-packages/scipy/sparse/_csr.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Compressed Sparse Row matrix\n",
       "\n",
       "This can be instantiated in several ways:\n",
       "    csr_array(D)\n",
       "        with a dense matrix or rank-2 ndarray D\n",
       "\n",
       "    csr_array(S)\n",
       "        with another sparse matrix S (equivalent to S.tocsr())\n",
       "\n",
       "    csr_array((M, N), [dtype])\n",
       "        to construct an empty matrix with shape (M, N)\n",
       "        dtype is optional, defaulting to dtype='d'.\n",
       "\n",
       "    csr_array((data, (row_ind, col_ind)), [shape=(M, N)])\n",
       "        where ``data``, ``row_ind`` and ``col_ind`` satisfy the\n",
       "        relationship ``a[row_ind[k], col_ind[k]] = data[k]``.\n",
       "\n",
       "    csr_array((data, indices, indptr), [shape=(M, N)])\n",
       "        is the standard CSR representation where the column indices for\n",
       "        row i are stored in ``indices[indptr[i]:indptr[i+1]]`` and their\n",
       "        corresponding values are stored in ``data[indptr[i]:indptr[i+1]]``.\n",
       "        If the shape parameter is not supplied, the matrix dimensions\n",
       "        are inferred from the index arrays.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "dtype : dtype\n",
       "    Data type of the matrix\n",
       "shape : 2-tuple\n",
       "    Shape of the matrix\n",
       "ndim : int\n",
       "    Number of dimensions (this is always 2)\n",
       "nnz\n",
       "    Number of stored values, including explicit zeros\n",
       "data\n",
       "    CSR format data array of the matrix\n",
       "indices\n",
       "    CSR format index array of the matrix\n",
       "indptr\n",
       "    CSR format index pointer array of the matrix\n",
       "has_sorted_indices\n",
       "    Whether indices are sorted\n",
       "\n",
       "Notes\n",
       "-----\n",
       "\n",
       "Sparse matrices can be used in arithmetic operations: they support\n",
       "addition, subtraction, multiplication, division, and matrix power.\n",
       "\n",
       "Advantages of the CSR format\n",
       "  - efficient arithmetic operations CSR + CSR, CSR * CSR, etc.\n",
       "  - efficient row slicing\n",
       "  - fast matrix vector products\n",
       "\n",
       "Disadvantages of the CSR format\n",
       "  - slow column slicing operations (consider CSC)\n",
       "  - changes to the sparsity structure are expensive (consider LIL or DOK)\n",
       "\n",
       "Canonical Format\n",
       "    - Within each row, indices are sorted by column.\n",
       "    - There are no duplicate entries.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> from scipy.sparse import csr_array\n",
       ">>> csr_array((3, 4), dtype=np.int8).toarray()\n",
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]], dtype=int8)\n",
       "\n",
       ">>> row = np.array([0, 0, 1, 2, 2, 2])\n",
       ">>> col = np.array([0, 2, 2, 0, 1, 2])\n",
       ">>> data = np.array([1, 2, 3, 4, 5, 6])\n",
       ">>> csr_array((data, (row, col)), shape=(3, 3)).toarray()\n",
       "array([[1, 0, 2],\n",
       "       [0, 0, 3],\n",
       "       [4, 5, 6]])\n",
       "\n",
       ">>> indptr = np.array([0, 2, 3, 6])\n",
       ">>> indices = np.array([0, 2, 2, 0, 1, 2])\n",
       ">>> data = np.array([1, 2, 3, 4, 5, 6])\n",
       ">>> csr_array((data, indices, indptr), shape=(3, 3)).toarray()\n",
       "array([[1, 0, 2],\n",
       "       [0, 0, 3],\n",
       "       [4, 5, 6]])\n",
       "\n",
       "Duplicate entries are summed together:\n",
       "\n",
       ">>> row = np.array([0, 1, 2, 0])\n",
       ">>> col = np.array([0, 1, 1, 0])\n",
       ">>> data = np.array([1, 2, 4, 8])\n",
       ">>> csr_array((data, (row, col)), shape=(3, 3)).toarray()\n",
       "array([[9, 0, 0],\n",
       "       [0, 2, 0],\n",
       "       [0, 4, 0]])\n",
       "\n",
       "As an example of how to construct a CSR matrix incrementally,\n",
       "the following snippet builds a term-document matrix from texts:\n",
       "\n",
       ">>> docs = [[\"hello\", \"world\", \"hello\"], [\"goodbye\", \"cruel\", \"world\"]]\n",
       ">>> indptr = [0]\n",
       ">>> indices = []\n",
       ">>> data = []\n",
       ">>> vocabulary = {}\n",
       ">>> for d in docs:\n",
       "...     for term in d:\n",
       "...         index = vocabulary.setdefault(term, len(vocabulary))\n",
       "...         indices.append(index)\n",
       "...         data.append(1)\n",
       "...     indptr.append(len(indices))\n",
       "...\n",
       ">>> csr_array((data, indices, indptr), dtype=int).toarray()\n",
       "array([[2, 1, 0, 0],\n",
       "       [0, 1, 1, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab962ae8-e09f-498f-917e-8ac45ac01441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f0a246d2-4474-40d2-9e14-1027eba86b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_array = doc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5dada753-eddf-4f99-94f3-1a39d05feaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8a525c6d-8f7a-4f72-8f01-9eeaefba27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent3 = [\"this is a good movie but actor is not present\"]\n",
    "sent1 = [\"this is good movie\"]\n",
    "sent2 = [\"this is not a good movie\"]\n",
    "# again It can confuse our classifier \n",
    "# as sentence 3 causing for sentence 2 also, as it can predict the movie as \n",
    "# a good one\n",
    "# for this we need bi grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e0959af5-5c6e-418b-bf99-2585f5cd6c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))# here it is bigram,\n",
    "#by default CountVectorizer is unigram feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "803921f7-a34e-4240-9faf-b7f4b53e9036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is good movie', 'this is not a good movie']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [sent1[0],sent2[0]]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "be1fb5cb-1256-4d3e-b399-fbe209a39b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = cv.fit_transform(docs)\n",
    "# now our output is chaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "399d539b-ec33-4299-82eb-afd7ef4f1d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0aa76cf7-8eb9-4326-b71f-5bbb9a6c6a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_array = docs.toarray()\n",
    "doc_array\n",
    "# both are different answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "17efe48b-9c78-47ef-9d74-cd786e1cf24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 4, 'is good': 1, 'good movie': 0, 'is not': 2, 'not good': 3}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n",
    "# here this is , is good are as a single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "292b2b3c-0ec7-4bff-82d3-81e7e0015446",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d5bef1ae-caaf-4b2c-9a74-d03a79238f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = [sent1[0],sent2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1446a828-13aa-4e37-8949-3af6949c6b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is good movie', 'this is not a good movie']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1ce78902-f4e9-4b6c-a369-1fc1368e676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = cv.fit_transform(doc3).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ff3b605e-3bed-489e-aeee-115f1a306377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "411ea4c1-c5d3-4453-9b3e-cc10e52423a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is good': 3,\n",
       " 'is good movie': 0,\n",
       " 'this is not': 4,\n",
       " 'is not good': 1,\n",
       " 'not good movie': 2}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_\n",
    "# now 3 words as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c99cf-5643-4773-8f08-ed87406e28b6",
   "metadata": {},
   "source": [
    "### Tf - idf Normalisation\n",
    "- Avoid features that occur very often , because they contain less information\n",
    "\n",
    "  \n",
    "- Information decreases as the number of occurence increases acorss  different type of documents\n",
    "\n",
    "  \n",
    "- So we define another term-term-document-frequency which associates a weight with every\n",
    "\n",
    "  \n",
    "-  for example sports occur several times in sport document , as the frequency of these words is more , less it is important\n",
    "\n",
    "  \n",
    "-  More frequent words contains less info more likely.\n",
    "\n",
    "  \n",
    "-  Example leader occurs very less time in politics but whenever it comes we know it is related to politics\n",
    "\n",
    "  \n",
    "-  TF-IDF is made up of two terms Term frequency and Inverse Document Frequency.\n",
    "\n",
    "  \n",
    "-  term frequency - tf(t,d) where t is term-frqency and d is document\n",
    "\n",
    "  \n",
    "-  idf - inverse document function(t,d) = log(N/(1+ count(t,D)),where  coutn(t,D) means how many time this term t occured across the entire document D, N is the total no of document.\n",
    "\n",
    "  \n",
    "-  for example \"good\" word appear thrice in the corpus which has 3 documents ie N =3 , count(t,D) =3{for a given document the term frequency is 1.\n",
    "-  hence log(1) = 0, which implies good word has very less contribution.\n",
    "   \n",
    "-  This implies , word significnce increases as the number start approaching towards the 1, ie word will have a higher weight\n",
    "-  weight will between 0 and 1.\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4500731c-24df-4f85-aff0-c05eda53b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"this is good movie\"\n",
    "sent2 = 'this is not a good movie'\n",
    "sent3 = 'this is a good movie but actor was not present'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fe7bf97c-e327-4ed1-9938-d35aa22f8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = [sent1,sent2,sent3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a3852f92-2118-4195-afba-bd0fe51bf514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "06fc48dd-6322-4681-9297-94220014b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "682594f7-b23e-4b82-89f1-e2a825dd6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorised_corpus = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6fff1fc6-02a2-4baf-9824-4c8c30d2f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorised_corpus_array = vectorised_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cf14b756-0e1e-490f-ae1d-f6bd97670286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.5        0.5        0.5        0.\n",
      "  0.         0.5        0.        ]\n",
      " [0.         0.         0.42040099 0.42040099 0.42040099 0.54134281\n",
      "  0.         0.42040099 0.        ]\n",
      " [0.40914568 0.40914568 0.24164803 0.24164803 0.24164803 0.31116583\n",
      "  0.40914568 0.24164803 0.40914568]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorised_corpus_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "162ee156-268a-43c4-a151-9c3e3edfcdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 7, 'is': 3, 'good': 2, 'movie': 4, 'not': 5, 'but': 1, 'actor': 0, 'was': 8, 'present': 6}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d98985-335d-4663-856d-e114349f21dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02646b-65ab-4106-bb21-665b2f5ea651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
