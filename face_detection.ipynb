{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc86af7-80e6-43d7-8179-a0152b9ce23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83e1fa-d23c-4fd1-882d-490a2e9707ef",
   "metadata": {},
   "source": [
    "### for code both  gray frame and frame\n",
    "- this code will give blue color rectangle of thickness 10 , A video running stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9665fc-b1f0-45dd-821c-437ab5ebdcbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_pressed_user \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m): \n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     33\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    return_value, frame = capture.read()\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if return_value == False :\n",
    "        continue\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray_frame, 1.3, 5)\n",
    "# 1.3 is scale factor - how much the image size is reduced at each image scale\n",
    "# 1.3 means image shrinks by 30%\n",
    "# 1.05 means image shrinks by 5 %\n",
    "# 5 is no. of neighbours-->> no. of neighbours each rectangles should have\n",
    "\n",
    "\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 255, 0), 10)\n",
    "#because in opencv we can make rectangle using two diagnally opposite points\n",
    "#(255,0,0 ) is color of boundary , 10 is thickness of  line(blue color, here)\n",
    "\n",
    "    cv2.imshow(\"Video Frame\", frame)\n",
    "\n",
    "    key_pressed_user =cv2.waitKey(1) & 0xFF \n",
    "    \n",
    "    if key_pressed_user == ord('q'): \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f4f0b3-4f67-4303-8c42-2ab8e24c80a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "rectangle(img, pt1, pt2, color[, thickness[, lineType[, shift]]]) -> img\n",
       ".   @brief Draws a simple, thick, or filled up-right rectangle.\n",
       ".   \n",
       ".   The function cv::rectangle draws a rectangle outline or a filled rectangle whose two opposite corners\n",
       ".   are pt1 and pt2.\n",
       ".   \n",
       ".   @param img Image.\n",
       ".   @param pt1 Vertex of the rectangle.\n",
       ".   @param pt2 Vertex of the rectangle opposite to pt1 .\n",
       ".   @param color Rectangle color or brightness (grayscale image).\n",
       ".   @param thickness Thickness of lines that make up the rectangle. Negative values, like #FILLED,\n",
       ".   mean that the function has to draw a filled rectangle.\n",
       ".   @param lineType Type of the line. See #LineTypes\n",
       ".   @param shift Number of fractional bits in the point coordinates.\n",
       "\n",
       "\n",
       "\n",
       "rectangle(img, rec, color[, thickness[, lineType[, shift]]]) -> img\n",
       ".   @overload\n",
       ".   \n",
       ".   use `rec` parameter as alternative specification of the drawn rectangle: `r.tl() and\n",
       ".   r.br()-Point(1,1)` are opposite corners\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv2.rectangle?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70377857-14c8-4b0d-a009-b7a5ab9a6f1d",
   "metadata": {},
   "source": [
    "### What we are going to do ?\n",
    "- Read and show video stream , capture images\n",
    "- Detect Faces and show bounding box using (haarcascade).\n",
    "- Flatten the largest face image and save in a numpy array- for that I need to convert it into gray scale image.\n",
    "- Repeat the above for multiple people to generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63db67d-50c2-4a88-952f-160e011e8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac0f8c0-6b8f-483d-ac41-5b316bb04415",
   "metadata": {},
   "source": [
    "### Initializing the Camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7882ba5-3acd-4d4c-9908-fc1cf05e852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0) # default position as only single stream "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3169114-b82c-4d7e-84f7-01f70b18d8eb",
   "metadata": {},
   "source": [
    "### Face Detection - loading of a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc156fc1-a782-478a-ae37-c08412ff236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305dac2-5332-442c-a48e-7581abbc4b2f",
   "metadata": {},
   "source": [
    "### Flatten the largest face image and save in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761dbc4d-ca55-40ea-b80f-af9fe2280a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of person : Om\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 00:35:38.661 python[5116:179184] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-09-29 00:35:38.661 python[5116:179184] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "(7, 30000)\n",
      "Data successfully saved at face_detection_folder/ Om.npy\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tempfile import TemporaryFile \n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "skip = 0 \n",
    "# storing every 10th face\n",
    "face_list = []\n",
    "dataset_path = 'face_detection_folder/' # creating a dataset which stores the data at this path\n",
    "file_name = input(\"Enter the name of person :\")   \n",
    "\n",
    "while True:\n",
    "    return_value, frame = capture.read()\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame , cv2.COLOR_BGR2GRAY) # to for face detection\n",
    "\n",
    "   \n",
    "    if return_value  == False:\n",
    "        continue\n",
    "    \n",
    "#face detection\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(frame, 1.3, 5)\n",
    "# it is a list of  faces where faces are  tuple  in form of (x,y,w,h) \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Now we will sort the faces, using area of the face which is w*h,\n",
    "\n",
    "    faces = sorted(faces, key = lambda f: f[2]*f[3])\n",
    "    \n",
    "#pick the largest face according to the area\n",
    "    for face in faces[-1:] :#in descending order for largest face\n",
    "        x, y, w, h = face\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+ h), (0, 255, 255), 10)\n",
    "        \n",
    "\n",
    "\n",
    "#Extract (crop out the required face) : also called Region of interest\n",
    "        \n",
    "        offset = 10 \n",
    "\n",
    "\n",
    "# pixels in each dirfection of rectangle adding\n",
    "        \n",
    "        face_section = frame[(y - offset):(y + h + offset), (x -offset): (x + w + offset)]\n",
    "        face_section = cv2.resize(face_section, (100, 100)) \n",
    "        # into a resized frame\n",
    "        skip = skip + 1 # when we do this we arecapturing the 10th frame each time \n",
    "        if (skip % 10 == 0):\n",
    "     \n",
    "            face_list.append(face_section)\n",
    "            print(len(face_list))\n",
    "            \n",
    "            \n",
    "        cv2.imshow(\"Face Section\", face_section)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "        \n",
    "# stopping the stream\n",
    "    key_pressed = cv2.waitKey(1) & 0xFF\n",
    "    if key_pressed == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# convert our face_list into a numpy array\n",
    "face_array = np.array(face_list)\n",
    "face_array = face_array.reshape((face_array.shape[0], -1))\n",
    "print(face_array.shape)\n",
    "\n",
    "\n",
    "#saving the data into file system\n",
    "np.save(dataset_path + file_name +\".npy\", face_array)\n",
    "# this is how we can save any file\n",
    "print(\"Data successfully saved at \" + dataset_path +\" \"+ file_name +\".npy\")\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "#output is no.  of frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758236de-6877-4665-9279-07c225730531",
   "metadata": {},
   "source": [
    "### Recognising Faces using some classification algorithm like \n",
    "- logistic\n",
    "- KNN\n",
    "- SVM\n",
    "### Process:\n",
    "- Read a video stream using opencv\n",
    "- extract the faces out of it for testing\n",
    "- load the training data (numpy arrays of all the persons)\n",
    "  1. x-values are stored in numpy arrays\n",
    "  2. y-values we need to assign  for each person\n",
    "- Use KNN to find the prediction of face(int)\n",
    "- map the prediction id to name of the user\n",
    "- Display the predictions on the screen - bounding box and name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681579e3-fdb3-43a3-876e-77236b74f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1562547f-11bf-4a3c-8409-7fd4295222f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(v1, v2):\n",
    "    #eucledian\n",
    "    return np.sqrt(((v1 - v2)**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42c7339-f62e-43e1-8840-df669e413f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(train,test, k = 5):\n",
    "    dist = []\n",
    "    for i in range(train.shape[0]):\n",
    "        #get the vector and label\n",
    "        ix = train[i, :-1]\n",
    "        iy = train[i, -1]\n",
    "        #Compute the distance from test point.\n",
    "    d = distance(test, ix)\n",
    "    dist.append([d, iy])\n",
    "    # sort based on distance and get top k\n",
    "    dk = sorted(dist, key = lambda x : x[0][:k])\n",
    "    #retrieve only the labels \n",
    "    labels = np.array(dk)[:, -1]\n",
    "    # Get frequencies of each label\n",
    "    output = np.unique(labels, return_counts = True)\n",
    "    #Find max frequency and corresponding label \n",
    "    index = np.argmax(output[1])\n",
    "    return output[0][index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3542d48-5ed0-4eeb-b309-211aaf3873bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fdc2e5b-b1ae-40cb-bb41-428dc4b24c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadedomkar12.npy\n",
      "Loadedomkar13.npy\n",
      "LoadedOm.npy\n",
      "(39, 30000)\n",
      "(46, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 39 and the array at index 1 has size 46",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(face_dataset\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(face_label\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 43\u001b[0m trainset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((face_dataset, face_label), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainset\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#testing \u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 39 and the array at index 1 has size 46"
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "skip = 0 \n",
    "dataset_path = 'face_detection_folder/'\n",
    "# storing every 10th face\n",
    "face_list = [] # training data\n",
    "label = []  #y values of data\n",
    "class_id = 0 # labels for the given file,it will start from zero and than increment \n",
    "names = {} # ,mapping btw id and  name\n",
    "\n",
    "\n",
    "\n",
    "#data Preparation - where fx is some random file\n",
    "for fx in os.listdir(dataset_path): # iterrating overmy own directory\n",
    "    if fx.endswith('.npy'):\n",
    "        names[class_id] = fx[:-4]#labelling the file name and remove .npy\n",
    "        print(\"Loaded\" + fx)# 4 chracters-->> .npy\n",
    "        data_item = np.load(dataset_path + fx)\n",
    "        face_list.append(data_item)\n",
    "\n",
    "# create labels for the class:\n",
    "# there are several files in our folder , each file have some faces of person and \n",
    "# it will bascially have 10 rows ,a some no. features (for example)\n",
    "# for all the x values we created the array of size 10\n",
    "# so, our target will be of shape 10 and will have a class id of 0 , as it is the first\n",
    "# first person , class id for second person weill be one and so on....\n",
    "# so for the first person we will get an array of zeros with 10 rows , similarly\n",
    "# second will have an multiplied each element with 1 and so on...\n",
    "# for each training point in each file we are computing one label using class_id\n",
    "# so basically , my data is files and my label is the outpur array recieved from\n",
    "# each file using class_id*np.ones((data_item.shape[0],))\n",
    "# So,there are multiple data sets[x1,x2,x3,#files] and multiples labels[[y1], [y2] ]\n",
    "# both of them are lsit , we will concatinate to make it a single list   \n",
    "    target = class_id*np.ones((data_item.shape[0],))\n",
    "    class_id += 1\n",
    "    label.append(target)\n",
    "face_dataset = np.concatenate(face_list, axis = 0) # all the x1,x2 ,x3 lists \n",
    "face_label   = np.concatenate(label, axis = 0).reshape((-1, 1))\n",
    "# all the corresponding targets\n",
    "print(face_dataset.shape)\n",
    "print(face_label.shape)\n",
    "\n",
    "trainset = np.concatenate((face_dataset, face_label), axis = 1)\n",
    "print(trainset.shape)\n",
    "\n",
    "#testing \n",
    "while True :\n",
    "    return_value , frame = capture.read()\n",
    "    if return_value == False:\n",
    "        continue\n",
    "    faces = face_cascade.detectMultiscale(frame, 1.3, 5)\n",
    "\n",
    "    for face in faces:\n",
    "        x,y,w,h = face\n",
    "        offset = 10\n",
    "        face_section = frame[(y - offset):(y + h + offset), (x -offset): (x + w + offset)]\n",
    "        face_section = cv2.resize(face_section, (100, 100)) \n",
    "\n",
    "        output = knn(trainset, face_section.flatten())\n",
    "\n",
    "        #display on the screen the name and  rectangle around it\n",
    "        prediction_name = name[int(out)]\n",
    "        cv2.putText(frame, prediction_name, (x, y- 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0))\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+ h ), (0, 255, 255), 2)\n",
    "            \n",
    "        \n",
    "    cv2.imshow(\"Faces\", frame)\n",
    "    \n",
    "    key_pressed = cv2.waitKey(1) & 0xFF\n",
    "    if key_pressed == ord('q'):\n",
    "        break\n",
    "capture.release()\n",
    "capture.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# KNN accepts a x data and y data together as 1 argument \n",
    "# So, we will concatenate x and y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3ec66-f3f4-4293-9340-5b34e28808d5",
   "metadata": {},
   "source": [
    "### Stopping the stream:\n",
    "- must be written in the program , otherwise videostream wont open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88902f6d-61f4-4fd1-8a92-8ebbd7f8f1b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (3750933369.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "key_pressed = cv2.waitKey(1) & 0xFF\n",
    "if key_pressed == ord('q'):\n",
    "    break\n",
    "capture.release()\n",
    "capture.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f2a1f-f706-4418-be64-03bc4e784b08",
   "metadata": {},
   "source": [
    "### video stream basic program to write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0929b676-dc6e-489d-8a32-7b2ba93a609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d610d5a-0fc2-47be-865e-6037b8d94316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 00:36:18.702 python[5155:180220] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-09-29 00:36:18.702 python[5155:180220] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "while True:\n",
    "    return_value, frame = capture.read()\n",
    "    \n",
    "\n",
    "    if return_value == False :\n",
    "        continue\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key_pressed_user =cv2.waitKey(1) & 0xFF \n",
    "    if key_pressed_user == ord('q'): \n",
    "        break\n",
    "    \n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a62c59-8846-4e24-a68b-1b66fd993531",
   "metadata": {},
   "source": [
    "### Programme to write face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1378e1fb-4514-42e1-ae3c-ec32da720207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[750 335 671 671]]\n",
      "[[752 339 671 671]]\n",
      "[[752 339 671 671]]\n",
      "[[750 341 666 666]]\n",
      "[[764 327 674 674]]\n",
      "[[765 329 674 674]]\n",
      "[[765 329 674 674]]\n",
      "[[768 329 678 678]]\n",
      "[[762 323 685 685]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 00:36:48.523 python[5167:180561] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-09-29 00:36:48.523 python[5167:180561] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[767 326 681 681]]\n",
      "[[763 318 691 691]]\n",
      "[[768 316 691 691]]\n",
      "[[772 321 682 682]]\n",
      "[[770 320 682 682]]\n",
      "[[775 327 681 681]]\n",
      "[[767 326 681 681]]\n",
      "[[767 326 681 681]]\n",
      "[[771 319 678 678]]\n",
      "[[776 326 666 666]]\n",
      "[[777 328 669 669]]\n",
      "[[768 326 678 678]]\n",
      "[[773 328 669 669]]\n",
      "[[780 333 663 663]]\n",
      "[[780 329 663 663]]\n",
      "[[782 329 666 666]]\n",
      "[[778 313 676 676]]\n",
      "[[769 294 692 692]]\n",
      "[[774 297 692 692]]\n",
      "[[778 290 692 692]]\n",
      "[[779 288 696 696]]\n",
      "[[773 289 696 696]]\n",
      "[[768 284 705 705]]\n",
      "[[773 292 696 696]]\n",
      "[[776 301 688 688]]\n",
      "[[778 301 692 692]]\n",
      "[[787 312 678 678]]\n",
      "[[782 310 686 686]]\n",
      "[[776 308 691 691]]\n",
      "[[778 308 686 686]]\n",
      "[[794 325 666 666]]\n",
      "[[784 317 681 681]]\n",
      "[[782 316 682 682]]\n",
      "[[784 317 681 681]]\n",
      "[[787 317 674 674]]\n",
      "[[782 308 684 684]]\n",
      "[[785 306 686 686]]\n",
      "[[784 310 681 681]]\n",
      "[[778 297 696 696]]\n",
      "[[778 297 696 696]]\n",
      "[[782 303 691 691]]\n",
      "[[781 303 687 687]]\n",
      "[[791 298 682 682]]\n",
      "[[787 299 684 684]]\n",
      "[[778 277 715 715]]\n",
      "[[791 293 687 687]]\n",
      "[[784 292 692 692]]\n",
      "[[790 292 686 686]]\n",
      "[[782 285 696 696]]\n",
      "[[788 288 691 691]]\n",
      "[[783 288 691 691]]\n",
      "[[782 285 696 696]]\n",
      "[[786 290 692 692]]\n",
      "[[788 295 692 692]]\n",
      "[[788 290 696 696]]\n",
      "[[792 297 684 684]]\n",
      "[[783 287 708 708]]\n",
      "[[797 294 694 694]]\n",
      "[[798 301 687 687]]\n",
      "[[798 312 670 670]]\n",
      "[[788 299 691 691]]\n",
      "[[793 305 685 685]]\n",
      "[[789 305 690 690]]\n",
      "[[793 316 681 681]]\n",
      "[[795 325 670 670]]\n",
      "[[794 329 671 671]]\n",
      "[[792 319 674 674]]\n",
      "[[795 321 674 674]]\n",
      "[[794 333 666 666]]\n",
      "[[785 320 681 681]]\n",
      "[[794 322 678 678]]\n",
      "[[798 320 670 670]]\n",
      "[[792 313 684 684]]\n",
      "[[788 299 690 690]]\n",
      "[[793 305 685 685]]\n",
      "[[788 299 691 691]]\n",
      "[[786 298 696 696]]\n",
      "[[788 299 691 691]]\n",
      "[[788 301 686 686]]\n",
      "[[786 301 691 691]]\n",
      "[[782 295 696 696]]\n",
      "[[797 308 676 676]]\n",
      "[[793 301 692 692]]\n",
      "[[788 299 696 696]]\n",
      "[[786 301 686 686]]\n",
      "[[791 302 686 686]]\n",
      "[[787 294 701 701]]\n",
      "[[787 294 702 702]]\n",
      "[[788 294 696 696]]\n",
      "[[790 286 706 706]]\n",
      "[[782 291 692 692]]\n",
      "[[767 284 693 693]]\n",
      "[[778 292 692 692]]\n",
      "[[788 295 684 684]]\n",
      "[[788 296 687 687]]\n",
      "[[783 293 687 687]]\n",
      "[[783 293 687 687]]\n",
      "[[785 290 692 692]]\n",
      "[[788 301 686 686]]\n",
      "[[788 301 686 686]]\n",
      "[[798 310 669 669]]\n",
      "[[788 295 692 692]]\n",
      "[[782 304 684 684]]\n",
      "[[773 297 688 688]]\n",
      "[[780 301 684 684]]\n",
      "[[765 304 684 684]]\n"
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "while True:\n",
    "    return_value, frame = capture.read()\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if return_value == False :\n",
    "        continue\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray_frame, 1.3, 5)\n",
    "    print(faces)\n",
    "\n",
    "\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 255, 0), 10)\n",
    "\n",
    "    cv2.imshow(\"Video Frame\", frame)\n",
    "    key_pressed_user =cv2.waitKey(1) & 0xFF \n",
    "    \n",
    "    if key_pressed_user == ord('q'): \n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ef61a-4d64-4a05-965a-36d37d03d48a",
   "metadata": {},
   "source": [
    "### Printing the frame , cropping of face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4aa8cd9-60ae-4e82-be1f-07ee41db9ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "capture = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "skip = 0 \n",
    "face_data = []\n",
    "dataset_path = 'face_detection_folder' \n",
    "    \n",
    "while True:\n",
    "    return_value, frame = capture.read()\n",
    "    gray_frame = cv2.cvtColor(frame , cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "   \n",
    "    if return_value  == False:\n",
    "        continue\n",
    "    faces = face_cascade.detectMultiScale(frame, 1.3, 5)\n",
    "    faces = sorted(faces, key = lambda f: f[2]*f[3])\n",
    "    \n",
    "\n",
    "    for face in faces[-1:] :\n",
    "        x, y, w, h = face\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+ h), (0, 255, 255), 10)\n",
    "        offset = 10 \n",
    "        face_section = frame[(y - offset):(y + h + offset), (x -offset): (x + w + offset)]\n",
    "        face_section = cv2.resize(face_section, (100, 100)) \n",
    "        \n",
    "        skip = skip + 1 \n",
    "        if(skip % 10 == 0):\n",
    "            face_data.append(face_section)\n",
    "            print(len(face_data))\n",
    "            \n",
    "        cv2.imshow(\"Face Section\", face_section)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key_pressed = cv2.waitKey(1) & 0xFF\n",
    "    if key_pressed == ord('q'):\n",
    "        break\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8219012-da0e-430d-a4a1-3a1b113d1c8a",
   "metadata": {},
   "source": [
    "### Learn to Save File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f3fb51-e8af-4b4b-8225-6b1acb51a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of person : om\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'face_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface_detection_folder\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m      2\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the name of person :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m np\u001b[38;5;241m.\u001b[39msave(dataset_path \u001b[38;5;241m+\u001b[39m file_name \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, face_array)\u001b[38;5;66;03m# save the data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData successfully saved at \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset_path \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m file_name \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'face_array' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_path = 'face_detection_folder' \n",
    "file_name = input(\"Enter the name of person :\")\n",
    "np.save(dataset_path + file_name +\".npy\", face_array)# save the data\n",
    "print(\"Data successfully saved at \" + dataset_path +\" \"+ file_name +\".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6879f1-55c8-4b8b-838a-97296cd1df96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
